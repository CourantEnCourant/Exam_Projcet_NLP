{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41887e5d-c2c2-474b-b7a7-bba5c4c05824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "# Check and display server config\n",
    "import server_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e60a09d-4e0d-4035-b766-48b6d95b0a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a74d14f84974e2a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T18:32:15.507829Z",
     "start_time": "2024-11-03T18:32:15.495428Z"
    }
   },
   "outputs": [],
   "source": [
    "# Configurations\n",
    "\n",
    "model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "model_cache_dir = \"../model\"\n",
    "\n",
    "src_lang = \"ja_XX\"\n",
    "target_lang = \"en_XX\"\n",
    "\n",
    "dataset_name = \"ryo0634/bsd_ja_en\"\n",
    "dataset_cache_dir = \"../data\"\n",
    "\n",
    "translations_save = \"../data/facebook_mbart.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T18:32:30.118669Z",
     "start_time": "2024-11-03T18:32:17.929435Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MBartForConditionalGeneration(\n",
       "  (model): MBartModel(\n",
       "    (shared): MBartScaledWordEmbedding(250054, 1024, padding_idx=1)\n",
       "    (encoder): MBartEncoder(\n",
       "      (embed_tokens): MBartScaledWordEmbedding(250054, 1024, padding_idx=1)\n",
       "      (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x MBartEncoderLayer(\n",
       "          (self_attn): MBartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): ReLU()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): MBartDecoder(\n",
       "      (embed_tokens): MBartScaledWordEmbedding(250054, 1024, padding_idx=1)\n",
       "      (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x MBartDecoderLayer(\n",
       "          (self_attn): MBartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MBartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=250054, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=model_cache_dir)\n",
    "tokenizer.src_lang = src_lang\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, cache_dir=model_cache_dir)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c8e080ed79f15da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T18:32:38.912554Z",
     "start_time": "2024-11-03T18:32:31.692906Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "ds = load_dataset(dataset_name, cache_dir=dataset_cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b61fd026802fd811",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T18:32:40.861662Z",
     "start_time": "2024-11-03T18:32:40.825253Z"
    }
   },
   "outputs": [],
   "source": [
    "# Merge datasets into one, remove unnecessary columns\n",
    "train = ds[\"train\"]\n",
    "test = ds[\"test\"]\n",
    "validation = ds[\"validation\"]\n",
    "\n",
    "ds_merged = concatenate_datasets([train, test, validation])\n",
    "ds_merged = ds_merged.filter(lambda example: example['original_language'] == 'ja')\n",
    "ds_merged = ds_merged.remove_columns(['id', 'tag', 'title', 'original_language', 'no', 'en_speaker', 'ja_speaker'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5122c8bf-b980-40d7-89b3-bf871319dc9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in Japanese: 12106\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of sentences in Japanese: {len(ds_merged)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95b55551cf325416",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T18:34:35.754425Z",
     "start_time": "2024-11-03T18:34:35.733402Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define tokenize functions\n",
    "from datasets import Dataset\n",
    "\n",
    "def tokenize_ja(example):\n",
    "    return tokenizer(example[\"ja_sentence\"], truncation=True, max_length=512, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "def tokenize_prepare(dataset: Dataset) -> Dataset:\n",
    "  tokenized_dataset = dataset.map(tokenize_ja, batched=True)\n",
    "  tokenized_dataset.set_format(\"torch\")\n",
    "  return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52abb0378fcf0971",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T18:34:46.237826Z",
     "start_time": "2024-11-03T18:34:36.324441Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f40fc508cd8441bb48458bf8182ba05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12106 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_tokenized = tokenize_prepare(ds_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95eb48ac2872874",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T18:36:10.583724Z",
     "start_time": "2024-11-03T18:34:49.241207Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f09212471bff484d8609df44cf150052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12106 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let the magic begin\n",
    "def generate_translation(batch):\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    outputs = model.generate(input_ids=input_ids, \n",
    "                             attention_mask=attention_mask, \n",
    "                             forced_bos_token_id=tokenizer.lang_code_to_id[target_lang])\n",
    "    batch[\"translations\"] = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "    return batch\n",
    "\n",
    "translated_dataset = ds_tokenized.map(generate_translation, batched=True, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c022405b4b1e141b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source sentence: 菓子パンやお茶なんかは買っておいてもいいですね。\n",
      "Translated sentence: You can buy some sweet bread and tea.\n",
      "Reference sentence: We can go buy some pastries and tea today.\n"
     ]
    }
   ],
   "source": [
    "# A qualitative test\n",
    "\n",
    "example = translated_dataset[1025]\n",
    "\n",
    "print(f\"Source sentence: {example['ja_sentence']}\")\n",
    "print(f\"Translated sentence: {example['translations']}\")\n",
    "print(f\"Reference sentence: {example['en_sentence']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9be36cd-0080-479f-8c00-cb3e62e2182a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning jobs\n",
    "translated_dataset = translated_dataset.remove_columns(['input_ids', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa8653ff-0f53-4401-a2c7-45a0f1b663bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4a85691415240059a103e572532a3b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/13 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2166787"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save tranlated dataset\n",
    "translated_dataset.to_parquet(translations_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fd722d-7526-4c58-a64d-fe8523769186",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "structured_document",
   "language": "python",
   "name": "structured_document"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
