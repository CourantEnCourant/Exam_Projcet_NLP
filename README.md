# Exam_Projcet_NLP
## Groupe Member
> [Siman CHEN] (https://github.com/simannnc)
> 
> [Weiqi ZHANG] (https://github.com/CourantEnCourant)
> 
> [Yuntian SHEN] (https://github.com/ShenYT0)
>
## Translation task from Japanese to English
We carried out a translation task from Japanese to English by comparing two models: opus-mt-ja-en and mbart-large-50-many-to-many-mmt, using a benchmark corpus.

## Selected Corpus
https://paperswithcode.com/dataset/business-scene-dialogue

## Selected Model
https://huggingface.co/Helsinki-NLP/opus-mt-ja-en
https://huggingface.co/facebook/mbart-large-50-many-to-many-mmt

## Results
The results indicate that although opus-mt-ja-en operates faster, the larger model, mbart-large-50-many-to-many-mmt, which is trained on a more extensive dataset, achieves higher scores and performs better.
