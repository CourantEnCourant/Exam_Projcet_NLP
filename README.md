# Exam_Projcet_NLP
## Groupe Member
> [Siman CHEN] (https://github.com/simannnc)
> 
> [Weiqi ZHANG] (https://github.com/CourantEnCourant)
> 
> [Yuntian SHEN] (https://github.com/ShenYT0)
>

## Selected Corpus
**Japanese-English Business Scene Dialogue parallel corpus**

https://paperswithcode.com/dataset/business-scene-dialogue

## Selected Model
https://huggingface.co/Helsinki-NLP/opus-mt-ja-en (For evaluation)

https://huggingface.co/facebook/mbart-large-50-many-to-many-mmt (For comparison)

## Corpus analysis

## Introducing the model

## Evaluating the corpus using two model

## Results
The results indicate that although opus-mt-ja-en operates faster, the larger model, mbart-large-50-many-to-many-mmt, which is trained on a more extensive dataset, achieves higher scores and performs better.